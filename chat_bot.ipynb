Open In Colab
from google.colab import drive
drive.mount('/content/drive')
Mounted at /content/drive
!pip install anvil-uplink
Collecting anvil-uplink
  Downloading anvil_uplink-0.3.39-py2.py3-none-any.whl (62 kB)
     |████████████████████████████████| 62 kB 823 kB/s 
Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from anvil-uplink) (1.15.0)
Collecting argparse
  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)
Collecting ws4py
  Downloading ws4py-0.5.1.tar.gz (51 kB)
     |████████████████████████████████| 51 kB 201 kB/s 
Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from anvil-uplink) (0.16.0)
Building wheels for collected packages: ws4py
  Building wheel for ws4py (setup.py) ... done
  Created wheel for ws4py: filename=ws4py-0.5.1-py3-none-any.whl size=45230 sha256=4ba90ee32343ea29a2a3f32438560190994977570ce49e35126bf3ba385315dc
  Stored in directory: /root/.cache/pip/wheels/29/ea/7d/3410aa0aa0e4402ead9a7a97ab2214804887e0f5c2b76f0c96
Successfully built ws4py
Installing collected packages: ws4py, argparse, anvil-uplink
Successfully installed anvil-uplink-0.3.39 argparse-1.4.0 ws4py-0.5.1
import anvil.server

anvil.server.connect("NC6JCRR6RSHGSOARZDFGN2Y4-KX6WX2KBRC7MDYSC")
Connecting to wss://anvil.works/uplink
Anvil websocket open
Connected to "Default environment (dev)" as SERVER
# pip install tensorflow keras pickle nltk
from google.colab import drive
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
# from keras.optimizers import SGD
from tensorflow.keras.optimizers import SGD
import random
import nltk
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
import json
import pickle
intents_file = open('/content/drive/MyDrive/Colab Notebooks/intents.json').read()
intents = json.loads(intents_file)
import nltk
nltk.download('punkt')
words=[]
classes = []
documents = []
ignore_letters = ['!', '?', ',', '.']
for intent in intents['intents']:
    for pattern in intent['patterns']:
        #tokenize each word
        word = nltk.word_tokenize(pattern)
        words.extend(word)
        #add documents in the corpus
        documents.append((word, intent['tag']))
        # add to our classes list
        if intent['tag'] not in classes:
            classes.append(intent['tag'])
print(documents)
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
[(['hi'], 'greeting'), (['hello'], 'greeting'), (['whats', 'up'], 'greeting'), (['sup'], 'greeting'), (['is', 'anyone', 'there'], 'greeting'), (['whats', 'good'], 'greeting'), (['hey'], 'greeting'), (['bye'], 'goodbye'), (['cya'], 'goodbye'), (['see', 'you', 'later'], 'goodbye'), (['goodbye'], 'goodbye'), (['im', 'leaving'], 'goodbye'), (['have', 'a', 'good', 'day'], 'goodbye'), (['how', 'old', 'are', 'you'], 'age'), (['what', 'is', 'your', 'age'], 'age'), (['thanks'], 'thanks'), (['thank', 'you'], 'thanks'), (['thankyou'], 'thanks'), (['ty'], 'thanks'), (['I', 'owe', 'you', 'one'], 'thanks'), (['whats', 'is', 'your', 'name'], 'name'), (['whats', 'your', 'name'], 'name'), (['whats', 'should', 'I', 'call', 'you'], 'name'), (['how', 'should', 'I', 'address', 'you'], 'name'), (['Yes', 'it', 'does'], 'sky_net_yes'), (['Yeah'], 'sky_net_yes'), (['Haha', 'yep'], 'sky_net_yes'), (['yes'], 'sky_net_yes'), (['Indeed'], 'sky_net_yes'), (['Yup'], 'sky_net_yes'), (['Just', 'like', 'the', 'terminator'], 'sky_net_yes'), (['no'], 'sky_net_no'), (['nah'], 'sky_net_no'), (['not', 'really'], 'sky_net_no'), (['thats', 'scary'], 'sky_net_no'), (['singularity'], 'sky_net_no'), (['how', 'are', 'you'], 'how_are_you'), (['how', 'are', 'you', 'doing'], 'how_are_you'), (['what', 'is', 'going', 'on'], 'how_are_you'), (['I', 'am', 'doing', 'great'], 'doing_great'), (['I', 'am', 'well'], 'doing_great'), (['Im', 'great'], 'doing_great'), (['awesome'], 'doing_great'), (['happy'], 'doing_great'), (['better'], 'doing_great'), (['not', 'great'], 'doing_badly'), (['not', 'well'], 'doing_badly'), (['not', 'good'], 'doing_badly'), (['bad'], 'doing_badly'), (['badly'], 'doing_badly'), (['terrible'], 'doing_badly'), (['horrible'], 'doing_badly'), (['awful'], 'doing_badly'), (['sad'], 'doing_badly'), (['wait', 'you', 'watch', 'Netflix'], 'netflix'), (['how', 'do', 'you', 'watch', 'Netflix'], 'netflix'), (['Netflix'], 'netflix'), (['how', 'can', 'you', 'run'], 'quick_run'), (['how', 'do', 'you', 'run'], 'quick_run'), (['how', 'run'], 'quick_run'), (['why', 'run'], 'quick_run'), (['run'], 'quick_run'), (['you', 'real'], 'real_bot'), (['you', 'human'], 'real_bot'), (['you', 'robot'], 'real_bot'), (['you', 'alive'], 'real_bot'), (['you', 'sentient'], 'real_bot'), (['you', 'conscious'], 'real_bot'), (['tell', 'me', 'joke'], 'joke'), (['got', 'any', 'good', 'jokes'], 'joke'), (['got', 'jokes'], 'joke'), (['can', 'you', 'tell', 'joke'], 'joke'), (['tell', 'joke'], 'joke'), (['haha'], 'good_joke'), (['that', 'was', 'funny'], 'good_joke'), (['very', 'funny'], 'good_joke'), (['good', 'one'], 'good_joke'), (['bad', 'joke'], 'bad_joke'), (['trash', 'joke'], 'bad_joke'), (['terrible'], 'bad_joke'), (['not', 'funny'], 'bad_joke'), (['I', 'hate', 'you'], 'hate'), (['you', 'stupid'], 'hate'), (['you', 'dumb'], 'hate'), (['you', 'mean'], 'hate'), (['you', 'my', 'friend'], 'like'), (['I', 'like', 'you'], 'like'), (['I', 'love', 'you'], 'like'), (['you', 'cool'], 'like'), (['you', 'are', 'chill'], 'like'), (['whats', 'favorite', 'show'], 'favorite_show'), (['favorite', 'tv', 'show'], 'favorite_show'), (['Whats', 'favorite', 'movie'], 'favorite_movie'), (['whats', 'favorite', 'film'], 'favorite_movie'), (['best', 'movie'], 'favorite_movie'), (['your', 'favorite', 'movie'], 'favorite_movie'), (['whats', 'favorite', 'movie'], 'favorite_movie'), (['What', 'think', 'about'], 'your_thoughts'), (['What', 'your', 'thoughts'], 'your_thoughts')]
import nltk
nltk.download('wordnet')
# create the training data
training = []
# create empty array for the output
output_empty = [0] * len(classes)
# training set, bag of words for every sentence
for doc in documents:
    # initializing bag of words

    bag = []
    # list of tokenized words for the pattern
    word_patterns = doc[0]
    # lemmatize each word - create base word, in attempt to represent related words
    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]
    # create the bag of words array with 1, if word is found in current pattern
    for word in words:
        bag.append(1) if word in word_patterns else bag.append(0)
    # output is a '0' for each tag and '1' for current tag (for each pattern)
    output_row = list(output_empty)
    output_row[classes.index(doc[1])] = 1
    training.append([bag, output_row])
# shuffle the features and make numpy array
random.shuffle(training)
training = np.array(training)
# create training and testing lists. X - patterns, Y - intents
train_x = list(training[:,0])
train_y = list(training[:,1])
print("Training data is created")
[nltk_data] Downloading package wordnet to /root/nltk_data...
[nltk_data]   Unzipping corpora/wordnet.zip.
Training data is created
/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
# deep neural networds model
model = Sequential()
model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]), activation='softmax'))
# Compiling model. SGD with Nesterov accelerated gradient gives good results for this model
sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
#Training and saving the model 
hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)
model.save('chatbot_model.h5', hist)
print("model is created")
/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  "The `lr` argument is deprecated, use `learning_rate` instead.")
Epoch 1/200
20/20 [==============================] - 1s 2ms/step - loss: 3.0831 - accuracy: 0.0707
Epoch 2/200
20/20 [==============================] - 0s 1ms/step - loss: 2.8746 - accuracy: 0.0808
Epoch 3/200
20/20 [==============================] - 0s 1ms/step - loss: 2.8169 - accuracy: 0.1818
Epoch 4/200
20/20 [==============================] - 0s 1ms/step - loss: 2.6969 - accuracy: 0.2626
Epoch 5/200
20/20 [==============================] - 0s 2ms/step - loss: 2.6612 - accuracy: 0.2020
Epoch 6/200
20/20 [==============================] - 0s 2ms/step - loss: 2.5733 - accuracy: 0.1919
Epoch 7/200
20/20 [==============================] - 0s 2ms/step - loss: 2.3531 - accuracy: 0.2525
Epoch 8/200
20/20 [==============================] - 0s 1ms/step - loss: 2.2559 - accuracy: 0.3333
Epoch 9/200
20/20 [==============================] - 0s 2ms/step - loss: 2.2536 - accuracy: 0.3333
Epoch 10/200
20/20 [==============================] - 0s 1ms/step - loss: 2.2195 - accuracy: 0.2929
Epoch 11/200
20/20 [==============================] - 0s 1ms/step - loss: 1.9786 - accuracy: 0.3131
Epoch 12/200
20/20 [==============================] - 0s 2ms/step - loss: 1.9616 - accuracy: 0.4444
Epoch 13/200
20/20 [==============================] - 0s 1ms/step - loss: 1.7797 - accuracy: 0.4242
Epoch 14/200
20/20 [==============================] - 0s 2ms/step - loss: 1.7140 - accuracy: 0.4141
Epoch 15/200
20/20 [==============================] - 0s 1ms/step - loss: 1.9798 - accuracy: 0.4141
Epoch 16/200
20/20 [==============================] - 0s 2ms/step - loss: 1.7023 - accuracy: 0.4343
Epoch 17/200
20/20 [==============================] - 0s 1ms/step - loss: 1.7264 - accuracy: 0.4545
Epoch 18/200
20/20 [==============================] - 0s 2ms/step - loss: 1.5838 - accuracy: 0.4343
Epoch 19/200
20/20 [==============================] - 0s 1ms/step - loss: 1.6309 - accuracy: 0.4545
Epoch 20/200
20/20 [==============================] - 0s 2ms/step - loss: 1.6902 - accuracy: 0.4141
Epoch 21/200
20/20 [==============================] - 0s 2ms/step - loss: 1.5834 - accuracy: 0.4747
Epoch 22/200
20/20 [==============================] - 0s 2ms/step - loss: 1.4780 - accuracy: 0.5354
Epoch 23/200
20/20 [==============================] - 0s 2ms/step - loss: 1.5284 - accuracy: 0.5253
Epoch 24/200
20/20 [==============================] - 0s 2ms/step - loss: 1.4970 - accuracy: 0.5354
Epoch 25/200
20/20 [==============================] - 0s 2ms/step - loss: 1.5123 - accuracy: 0.4949
Epoch 26/200
20/20 [==============================] - 0s 2ms/step - loss: 1.2486 - accuracy: 0.5960
Epoch 27/200
20/20 [==============================] - 0s 1ms/step - loss: 1.3617 - accuracy: 0.5758
Epoch 28/200
20/20 [==============================] - 0s 2ms/step - loss: 1.2256 - accuracy: 0.5960
Epoch 29/200
20/20 [==============================] - 0s 2ms/step - loss: 1.3481 - accuracy: 0.5556
Epoch 30/200
20/20 [==============================] - 0s 2ms/step - loss: 1.3179 - accuracy: 0.5960
Epoch 31/200
20/20 [==============================] - 0s 2ms/step - loss: 1.1740 - accuracy: 0.5657
Epoch 32/200
20/20 [==============================] - 0s 1ms/step - loss: 1.1590 - accuracy: 0.6162
Epoch 33/200
20/20 [==============================] - 0s 1ms/step - loss: 1.1111 - accuracy: 0.6667
Epoch 34/200
20/20 [==============================] - 0s 1ms/step - loss: 1.2297 - accuracy: 0.6061
Epoch 35/200
20/20 [==============================] - 0s 2ms/step - loss: 0.8904 - accuracy: 0.6970
Epoch 36/200
20/20 [==============================] - 0s 1ms/step - loss: 1.1082 - accuracy: 0.7172
Epoch 37/200
20/20 [==============================] - 0s 2ms/step - loss: 1.0066 - accuracy: 0.7071
Epoch 38/200
20/20 [==============================] - 0s 1ms/step - loss: 1.0185 - accuracy: 0.6263
Epoch 39/200
20/20 [==============================] - 0s 2ms/step - loss: 0.9903 - accuracy: 0.6364
Epoch 40/200
20/20 [==============================] - 0s 1ms/step - loss: 0.9819 - accuracy: 0.6162
Epoch 41/200
20/20 [==============================] - 0s 2ms/step - loss: 0.8309 - accuracy: 0.7374
Epoch 42/200
20/20 [==============================] - 0s 2ms/step - loss: 0.8442 - accuracy: 0.7273
Epoch 43/200
20/20 [==============================] - 0s 2ms/step - loss: 0.8072 - accuracy: 0.7677
Epoch 44/200
20/20 [==============================] - 0s 1ms/step - loss: 1.1490 - accuracy: 0.6162
Epoch 45/200
20/20 [==============================] - 0s 1ms/step - loss: 0.9463 - accuracy: 0.6667
Epoch 46/200
20/20 [==============================] - 0s 1ms/step - loss: 0.8780 - accuracy: 0.7172
Epoch 47/200
20/20 [==============================] - 0s 1ms/step - loss: 0.8982 - accuracy: 0.7071
Epoch 48/200
20/20 [==============================] - 0s 2ms/step - loss: 0.9322 - accuracy: 0.7172
Epoch 49/200
20/20 [==============================] - 0s 2ms/step - loss: 0.9244 - accuracy: 0.7172
Epoch 50/200
20/20 [==============================] - 0s 2ms/step - loss: 0.8727 - accuracy: 0.6970
Epoch 51/200
20/20 [==============================] - 0s 2ms/step - loss: 0.9178 - accuracy: 0.6263
Epoch 52/200
20/20 [==============================] - 0s 1ms/step - loss: 0.8359 - accuracy: 0.7172
Epoch 53/200
20/20 [==============================] - 0s 2ms/step - loss: 0.8791 - accuracy: 0.6667
Epoch 54/200
20/20 [==============================] - 0s 2ms/step - loss: 0.7121 - accuracy: 0.7273
Epoch 55/200
20/20 [==============================] - 0s 2ms/step - loss: 0.9251 - accuracy: 0.6465
Epoch 56/200
20/20 [==============================] - 0s 2ms/step - loss: 0.8239 - accuracy: 0.6869
Epoch 57/200
20/20 [==============================] - 0s 2ms/step - loss: 0.7132 - accuracy: 0.8081
Epoch 58/200
20/20 [==============================] - 0s 2ms/step - loss: 0.6695 - accuracy: 0.7879
Epoch 59/200
20/20 [==============================] - 0s 1ms/step - loss: 0.7563 - accuracy: 0.7374
Epoch 60/200
20/20 [==============================] - 0s 1ms/step - loss: 0.7566 - accuracy: 0.7778
Epoch 61/200
20/20 [==============================] - 0s 2ms/step - loss: 0.7840 - accuracy: 0.7273
Epoch 62/200
20/20 [==============================] - 0s 1ms/step - loss: 0.7457 - accuracy: 0.7374
Epoch 63/200
20/20 [==============================] - 0s 2ms/step - loss: 0.8732 - accuracy: 0.6970
Epoch 64/200
20/20 [==============================] - 0s 1ms/step - loss: 0.7415 - accuracy: 0.7172
Epoch 65/200
20/20 [==============================] - 0s 1ms/step - loss: 0.6295 - accuracy: 0.7778
Epoch 66/200
20/20 [==============================] - 0s 2ms/step - loss: 0.7864 - accuracy: 0.7071
Epoch 67/200
20/20 [==============================] - 0s 1ms/step - loss: 0.6703 - accuracy: 0.7576
Epoch 68/200
20/20 [==============================] - 0s 1ms/step - loss: 0.5871 - accuracy: 0.8081
Epoch 69/200
20/20 [==============================] - 0s 2ms/step - loss: 0.7459 - accuracy: 0.7374
Epoch 70/200
20/20 [==============================] - 0s 1ms/step - loss: 0.6288 - accuracy: 0.7778
Epoch 71/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5966 - accuracy: 0.8081
Epoch 72/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5830 - accuracy: 0.7980
Epoch 73/200
20/20 [==============================] - 0s 1ms/step - loss: 0.5919 - accuracy: 0.7677
Epoch 74/200
20/20 [==============================] - 0s 2ms/step - loss: 0.7052 - accuracy: 0.7475
Epoch 75/200
20/20 [==============================] - 0s 1ms/step - loss: 0.6731 - accuracy: 0.7576
Epoch 76/200
20/20 [==============================] - 0s 2ms/step - loss: 0.6234 - accuracy: 0.7677
Epoch 77/200
20/20 [==============================] - 0s 2ms/step - loss: 0.6792 - accuracy: 0.7576
Epoch 78/200
20/20 [==============================] - 0s 2ms/step - loss: 0.7430 - accuracy: 0.7677
Epoch 79/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5632 - accuracy: 0.8182
Epoch 80/200
20/20 [==============================] - 0s 2ms/step - loss: 0.6510 - accuracy: 0.7576
Epoch 81/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5607 - accuracy: 0.8182
Epoch 82/200
20/20 [==============================] - 0s 1ms/step - loss: 0.6138 - accuracy: 0.7576
Epoch 83/200
20/20 [==============================] - 0s 1ms/step - loss: 0.6247 - accuracy: 0.7475
Epoch 84/200
20/20 [==============================] - 0s 2ms/step - loss: 0.6343 - accuracy: 0.7980
Epoch 85/200
20/20 [==============================] - 0s 1ms/step - loss: 0.7009 - accuracy: 0.7475
Epoch 86/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5540 - accuracy: 0.8283
Epoch 87/200
20/20 [==============================] - 0s 1ms/step - loss: 0.5549 - accuracy: 0.7879
Epoch 88/200
20/20 [==============================] - 0s 1ms/step - loss: 0.6504 - accuracy: 0.7475
Epoch 89/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5817 - accuracy: 0.7879
Epoch 90/200
20/20 [==============================] - 0s 1ms/step - loss: 0.5699 - accuracy: 0.7778
Epoch 91/200
20/20 [==============================] - 0s 1ms/step - loss: 0.6480 - accuracy: 0.7879
Epoch 92/200
20/20 [==============================] - 0s 1ms/step - loss: 0.6164 - accuracy: 0.7576
Epoch 93/200
20/20 [==============================] - 0s 1ms/step - loss: 0.5841 - accuracy: 0.7778
Epoch 94/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.8384
Epoch 95/200
20/20 [==============================] - 0s 1ms/step - loss: 0.7140 - accuracy: 0.7879
Epoch 96/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5821 - accuracy: 0.7576
Epoch 97/200
20/20 [==============================] - 0s 2ms/step - loss: 0.7431 - accuracy: 0.7677
Epoch 98/200
20/20 [==============================] - 0s 2ms/step - loss: 0.6127 - accuracy: 0.7374
Epoch 99/200
20/20 [==============================] - 0s 1ms/step - loss: 0.5968 - accuracy: 0.7778
Epoch 100/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5640 - accuracy: 0.7778
Epoch 101/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4929 - accuracy: 0.7980
Epoch 102/200
20/20 [==============================] - 0s 1ms/step - loss: 0.6127 - accuracy: 0.7778
Epoch 103/200
20/20 [==============================] - 0s 2ms/step - loss: 0.7001 - accuracy: 0.7475
Epoch 104/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5070 - accuracy: 0.7778
Epoch 105/200
20/20 [==============================] - 0s 2ms/step - loss: 0.6076 - accuracy: 0.7475
Epoch 106/200
20/20 [==============================] - 0s 1ms/step - loss: 0.6417 - accuracy: 0.7778
Epoch 107/200
20/20 [==============================] - 0s 2ms/step - loss: 0.6856 - accuracy: 0.7475
Epoch 108/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5751 - accuracy: 0.8081
Epoch 109/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5055 - accuracy: 0.8586
Epoch 110/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4217 - accuracy: 0.8687
Epoch 111/200
20/20 [==============================] - 0s 1ms/step - loss: 0.7444 - accuracy: 0.8081
Epoch 112/200
20/20 [==============================] - 0s 1ms/step - loss: 0.5597 - accuracy: 0.8081
Epoch 113/200
20/20 [==============================] - 0s 1ms/step - loss: 0.5320 - accuracy: 0.7980
Epoch 114/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4723 - accuracy: 0.8081
Epoch 115/200
20/20 [==============================] - 0s 1ms/step - loss: 0.6494 - accuracy: 0.7778
Epoch 116/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5777 - accuracy: 0.7374
Epoch 117/200
20/20 [==============================] - 0s 1ms/step - loss: 0.6223 - accuracy: 0.7677
Epoch 118/200
20/20 [==============================] - 0s 1ms/step - loss: 0.5249 - accuracy: 0.8283
Epoch 119/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5812 - accuracy: 0.7778
Epoch 120/200
20/20 [==============================] - 0s 2ms/step - loss: 0.6147 - accuracy: 0.8283
Epoch 121/200
20/20 [==============================] - 0s 1ms/step - loss: 0.5939 - accuracy: 0.7879
Epoch 122/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5251 - accuracy: 0.8081
Epoch 123/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5815 - accuracy: 0.7980
Epoch 124/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5483 - accuracy: 0.7677
Epoch 125/200
20/20 [==============================] - 0s 1ms/step - loss: 0.4989 - accuracy: 0.7879
Epoch 126/200
20/20 [==============================] - 0s 1ms/step - loss: 0.4687 - accuracy: 0.8182
Epoch 127/200
20/20 [==============================] - 0s 1ms/step - loss: 0.5909 - accuracy: 0.7677
Epoch 128/200
20/20 [==============================] - 0s 2ms/step - loss: 0.6075 - accuracy: 0.8081
Epoch 129/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5377 - accuracy: 0.8384
Epoch 130/200
20/20 [==============================] - 0s 1ms/step - loss: 0.5024 - accuracy: 0.7980
Epoch 131/200
20/20 [==============================] - 0s 1ms/step - loss: 0.4855 - accuracy: 0.8283
Epoch 132/200
20/20 [==============================] - 0s 1ms/step - loss: 0.6719 - accuracy: 0.7374
Epoch 133/200
20/20 [==============================] - 0s 1ms/step - loss: 0.4238 - accuracy: 0.8384
Epoch 134/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5654 - accuracy: 0.7677
Epoch 135/200
20/20 [==============================] - 0s 1ms/step - loss: 0.5228 - accuracy: 0.7879
Epoch 136/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4684 - accuracy: 0.7980
Epoch 137/200
20/20 [==============================] - 0s 1ms/step - loss: 0.4296 - accuracy: 0.8384
Epoch 138/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5613 - accuracy: 0.7879
Epoch 139/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4625 - accuracy: 0.8384
Epoch 140/200
20/20 [==============================] - 0s 1ms/step - loss: 0.4929 - accuracy: 0.8586
Epoch 141/200
20/20 [==============================] - 0s 2ms/step - loss: 0.7027 - accuracy: 0.7778
Epoch 142/200
20/20 [==============================] - 0s 1ms/step - loss: 0.4502 - accuracy: 0.7980
Epoch 143/200
20/20 [==============================] - 0s 1ms/step - loss: 0.4463 - accuracy: 0.8182
Epoch 144/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4746 - accuracy: 0.8283
Epoch 145/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4465 - accuracy: 0.7980
Epoch 146/200
20/20 [==============================] - 0s 2ms/step - loss: 0.6268 - accuracy: 0.7879
Epoch 147/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5696 - accuracy: 0.7980
Epoch 148/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5256 - accuracy: 0.8384
Epoch 149/200
20/20 [==============================] - 0s 1ms/step - loss: 0.4918 - accuracy: 0.8081
Epoch 150/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5842 - accuracy: 0.7980
Epoch 151/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4848 - accuracy: 0.7879
Epoch 152/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5876 - accuracy: 0.8182
Epoch 153/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4631 - accuracy: 0.8485
Epoch 154/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7980
Epoch 155/200
20/20 [==============================] - 0s 2ms/step - loss: 0.6339 - accuracy: 0.7980
Epoch 156/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5776 - accuracy: 0.8283
Epoch 157/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5682 - accuracy: 0.7980
Epoch 158/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4414 - accuracy: 0.8283
Epoch 159/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5804 - accuracy: 0.7879
Epoch 160/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4475 - accuracy: 0.7879
Epoch 161/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4349 - accuracy: 0.8485
Epoch 162/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4666 - accuracy: 0.8485
Epoch 163/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4251 - accuracy: 0.8182
Epoch 164/200
20/20 [==============================] - 0s 1ms/step - loss: 0.4263 - accuracy: 0.8182
Epoch 165/200
20/20 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.8283
Epoch 166/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4358 - accuracy: 0.7980
Epoch 167/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5169 - accuracy: 0.7980
Epoch 168/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4899 - accuracy: 0.8283
Epoch 169/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4148 - accuracy: 0.8687
Epoch 170/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4275 - accuracy: 0.8283
Epoch 171/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.8687
Epoch 172/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4785 - accuracy: 0.8283
Epoch 173/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5425 - accuracy: 0.7980
Epoch 174/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5881 - accuracy: 0.7980
Epoch 175/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.8283
Epoch 176/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4477 - accuracy: 0.8384
Epoch 177/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4548 - accuracy: 0.8283
Epoch 178/200
20/20 [==============================] - 0s 2ms/step - loss: 0.6482 - accuracy: 0.7778
Epoch 179/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4115 - accuracy: 0.8081
Epoch 180/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5584 - accuracy: 0.8283
Epoch 181/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5775 - accuracy: 0.7778
Epoch 182/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4712 - accuracy: 0.8485
Epoch 183/200
20/20 [==============================] - 0s 1ms/step - loss: 0.5195 - accuracy: 0.8283
Epoch 184/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.8081
Epoch 185/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5307 - accuracy: 0.7879
Epoch 186/200
20/20 [==============================] - 0s 2ms/step - loss: 0.6057 - accuracy: 0.8081
Epoch 187/200
20/20 [==============================] - 0s 2ms/step - loss: 0.3904 - accuracy: 0.8687
Epoch 188/200
20/20 [==============================] - 0s 1ms/step - loss: 0.5449 - accuracy: 0.8081
Epoch 189/200
20/20 [==============================] - 0s 2ms/step - loss: 0.3702 - accuracy: 0.8384
Epoch 190/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5413 - accuracy: 0.8081
Epoch 191/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4780 - accuracy: 0.8485
Epoch 192/200
20/20 [==============================] - 0s 1ms/step - loss: 0.4283 - accuracy: 0.8182
Epoch 193/200
20/20 [==============================] - 0s 2ms/step - loss: 0.3972 - accuracy: 0.8687
Epoch 194/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4311 - accuracy: 0.7778
Epoch 195/200
20/20 [==============================] - 0s 1ms/step - loss: 0.4990 - accuracy: 0.8283
Epoch 196/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5920 - accuracy: 0.7879
Epoch 197/200
20/20 [==============================] - 0s 2ms/step - loss: 0.3970 - accuracy: 0.8182
Epoch 198/200
20/20 [==============================] - 0s 2ms/step - loss: 0.4668 - accuracy: 0.8384
Epoch 199/200
20/20 [==============================] - 0s 2ms/step - loss: 0.5232 - accuracy: 0.8081
Epoch 200/200
20/20 [==============================] - 0s 1ms/step - loss: 0.5063 - accuracy: 0.8182
model is created
import nltk
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
import pickle
import numpy as np
from keras.models import load_model
model = load_model('chatbot_model.h5')
import json
import random
intents = json.loads(open('/content/drive/MyDrive/Colab Notebooks/intents.json').read())
words = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/words.pkl','rb'))
classes = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/classes.pkl','rb'))
def clean_up_sentence(sentence):
    # tokenize the pattern - splitting words into array
    sentence_words = nltk.word_tokenize(sentence)
    # stemming every word - reducing to base form
    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]
    return sentence_words
# return bag of words array: 0 or 1 for words that exist in sentence
def bag_of_words(sentence, words, show_details=True):
    # tokenizing patterns
    sentence_words = clean_up_sentence(sentence)
    # bag of words - vocabulary matrix
    bag = [0]*len(words)  
    for s in sentence_words:
        for i,word in enumerate(words):
            if word == s: 
                # assign 1 if current word is in the vocabulary position
                bag[i] = 1
                if show_details:
                    print ("found in bag: %s" % word)
    return(np.array(bag))
def predict_class(sentence):
    # filter below  threshold predictions
    p = bag_of_words(sentence, words,show_details=False)
    res = model.predict(np.array([p]))[0]
    ERROR_THRESHOLD = 0.25
    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]
    # sorting strength probability
    results.sort(key=lambda x: x[1], reverse=True)
    return_list = []
    for r in results:
        return_list.append({"intent": classes[r[0]], "probability": str(r[1])})
    return return_list
def getResponse(ints, intents_json):
    tag = ints[0]['intent']
    list_of_intents = intents_json['intents']
    for i in list_of_intents:
        if(i['tag']== tag):
            result = random.choice(i['responses'])
            break
    return result
#Creating tkinter GUI
# import tkinter
# from tkinter import *
# def send():
#     msg = EntryBox.get("1.0",'end-1c').strip()
#     EntryBox.delete("0.0",END)
#     if msg != '':
#         ChatBox.config(state=NORMAL)
#         ChatBox.insert(END, "You: " + msg + '\n\n')
#         ChatBox.config(foreground="#446665", font=("Verdana", 12 )) 
#         ints = predict_class(msg)
#         res = getResponse(ints, intents)
#         ChatBox.insert(END, "Bot: " + res + '\n\n')           
#         ChatBox.config(state=DISABLED)
#         ChatBox.yview(END)
# root = Tk()
# root.title("Chatbot")
# root.geometry("400x500")
# root.resizable(width=FALSE, height=FALSE)
# #Create Chat window
# ChatBox = Text(root, bd=0, bg="white", height="8", width="50", font="Arial",)
# ChatBox.config(state=DISABLED)
# #Bind scrollbar to Chat window
# scrollbar = Scrollbar(root, command=ChatBox.yview, cursor="heart")
# ChatBox['yscrollcommand'] = scrollbar.set
# #Create Button to send message
# SendButton = Button(root, font=("Verdana",12,'bold'), text="Send", width="12", height=5,
#                     bd=0, bg="#f9a602", activebackground="#3c9d9b",fg='#000000',
#                     command= send )
# #Create the box to enter message
# EntryBox = Text(root, bd=0, bg="white",width="29", height="5", font="Arial")
# #EntryBox.bind("<Return>", send)
# #Place all components on the screen
# scrollbar.place(x=376,y=6, height=386)
# ChatBox.place(x=6,y=6, height=386, width=370)
# EntryBox.place(x=128, y=401, height=90, width=265)
# SendButton.place(x=6, y=401, height=90)
# root.mainloop()
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
# from keras.optimizers import SGD
from tensorflow.keras.optimizers import SGD
import random

import nltk
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
import json
import pickle

words=[]
classes = []
documents = []
ignore_letters = ['!', '?', ',', '.']
intents_file = open('/content/drive/MyDrive/Colab Notebooks/intents.json').read()
intents = json.loads(intents_file)

for intent in intents['intents']:
    for pattern in intent['patterns']:
        #tokenize each word
        word = nltk.word_tokenize(pattern)
        words.extend(word)
        #add documents in the corpus
        documents.append((word, intent['tag']))
        # add to our classes list
        if intent['tag'] not in classes:
            classes.append(intent['tag'])
print(documents)
# lemmaztize and lower each word and remove duplicates
words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_letters]
words = sorted(list(set(words)))
# sort classes
classes = sorted(list(set(classes)))
# documents = combination between patterns and intents
print (len(documents), "documents")
# classes = intents
print (len(classes), "classes", classes)
# words = all words, vocabulary
print (len(words), "unique lemmatized words", words)

pickle.dump(words,open('words.pkl','wb'))
pickle.dump(classes,open('classes.pkl','wb'))

# create our training data
training = []
# create an empty array for our output
output_empty = [0] * len(classes)
# training set, bag of words for each sentence
for doc in documents:
    # initialize our bag of words
    bag = []
    # list of tokenized words for the pattern
    pattern_words = doc[0]
    # lemmatize each word - create base word, in attempt to represent related words
    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]
    # create our bag of words array with 1, if word match found in current pattern
    for word in words:
        bag.append(1) if word in pattern_words else bag.append(0)
        
    # output is a '0' for each tag and '1' for current tag (for each pattern)
    output_row = list(output_empty)
    output_row[classes.index(doc[1])] = 1
    
    training.append([bag, output_row])
# shuffle our features and turn into np.array
random.shuffle(training)
training = np.array(training)
# create train and test lists. X - patterns, Y - intents
train_x = list(training[:,0])
train_y = list(training[:,1])
print("Training data created")

# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons
# equal to number of intents to predict output intent with softmax
model = Sequential()
model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]), activation='softmax'))

# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model
sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

#fitting and saving the model 
hist = model.fit(np.array(train_x), np.array(train_y), epochs=172, batch_size=5, verbose=1)
model.save('chatbot_model.h5', hist)

print("model created")
[(['hi'], 'greeting'), (['hello'], 'greeting'), (['whats', 'up'], 'greeting'), (['sup'], 'greeting'), (['is', 'anyone', 'there'], 'greeting'), (['whats', 'good'], 'greeting'), (['hey'], 'greeting'), (['bye'], 'goodbye'), (['cya'], 'goodbye'), (['see', 'you', 'later'], 'goodbye'), (['goodbye'], 'goodbye'), (['im', 'leaving'], 'goodbye'), (['have', 'a', 'good', 'day'], 'goodbye'), (['how', 'old', 'are', 'you'], 'age'), (['what', 'is', 'your', 'age'], 'age'), (['thanks'], 'thanks'), (['thank', 'you'], 'thanks'), (['thankyou'], 'thanks'), (['ty'], 'thanks'), (['I', 'owe', 'you', 'one'], 'thanks'), (['whats', 'is', 'your', 'name'], 'name'), (['whats', 'your', 'name'], 'name'), (['whats', 'should', 'I', 'call', 'you'], 'name'), (['how', 'should', 'I', 'address', 'you'], 'name'), (['Yes', 'it', 'does'], 'sky_net_yes'), (['Yeah'], 'sky_net_yes'), (['Haha', 'yep'], 'sky_net_yes'), (['yes'], 'sky_net_yes'), (['Indeed'], 'sky_net_yes'), (['Yup'], 'sky_net_yes'), (['Just', 'like', 'the', 'terminator'], 'sky_net_yes'), (['no'], 'sky_net_no'), (['nah'], 'sky_net_no'), (['not', 'really'], 'sky_net_no'), (['thats', 'scary'], 'sky_net_no'), (['singularity'], 'sky_net_no'), (['how', 'are', 'you'], 'how_are_you'), (['how', 'are', 'you', 'doing'], 'how_are_you'), (['what', 'is', 'going', 'on'], 'how_are_you'), (['I', 'am', 'doing', 'great'], 'doing_great'), (['I', 'am', 'well'], 'doing_great'), (['Im', 'great'], 'doing_great'), (['awesome'], 'doing_great'), (['happy'], 'doing_great'), (['better'], 'doing_great'), (['not', 'great'], 'doing_badly'), (['not', 'well'], 'doing_badly'), (['not', 'good'], 'doing_badly'), (['bad'], 'doing_badly'), (['badly'], 'doing_badly'), (['terrible'], 'doing_badly'), (['horrible'], 'doing_badly'), (['awful'], 'doing_badly'), (['sad'], 'doing_badly'), (['wait', 'you', 'watch', 'Netflix'], 'netflix'), (['how', 'do', 'you', 'watch', 'Netflix'], 'netflix'), (['Netflix'], 'netflix'), (['how', 'can', 'you', 'run'], 'quick_run'), (['how', 'do', 'you', 'run'], 'quick_run'), (['how', 'run'], 'quick_run'), (['why', 'run'], 'quick_run'), (['run'], 'quick_run'), (['you', 'real'], 'real_bot'), (['you', 'human'], 'real_bot'), (['you', 'robot'], 'real_bot'), (['you', 'alive'], 'real_bot'), (['you', 'sentient'], 'real_bot'), (['you', 'conscious'], 'real_bot'), (['tell', 'me', 'joke'], 'joke'), (['got', 'any', 'good', 'jokes'], 'joke'), (['got', 'jokes'], 'joke'), (['can', 'you', 'tell', 'joke'], 'joke'), (['tell', 'joke'], 'joke'), (['haha'], 'good_joke'), (['that', 'was', 'funny'], 'good_joke'), (['very', 'funny'], 'good_joke'), (['good', 'one'], 'good_joke'), (['bad', 'joke'], 'bad_joke'), (['trash', 'joke'], 'bad_joke'), (['terrible'], 'bad_joke'), (['not', 'funny'], 'bad_joke'), (['I', 'hate', 'you'], 'hate'), (['you', 'stupid'], 'hate'), (['you', 'dumb'], 'hate'), (['you', 'mean'], 'hate'), (['you', 'my', 'friend'], 'like'), (['I', 'like', 'you'], 'like'), (['I', 'love', 'you'], 'like'), (['you', 'cool'], 'like'), (['you', 'are', 'chill'], 'like'), (['whats', 'favorite', 'show'], 'favorite_show'), (['favorite', 'tv', 'show'], 'favorite_show'), (['Whats', 'favorite', 'movie'], 'favorite_movie'), (['whats', 'favorite', 'film'], 'favorite_movie'), (['best', 'movie'], 'favorite_movie'), (['your', 'favorite', 'movie'], 'favorite_movie'), (['whats', 'favorite', 'movie'], 'favorite_movie'), (['What', 'think', 'about'], 'your_thoughts'), (['What', 'your', 'thoughts'], 'your_thoughts')]
99 documents
21 classes ['age', 'bad_joke', 'doing_badly', 'doing_great', 'favorite_movie', 'favorite_show', 'good_joke', 'goodbye', 'greeting', 'hate', 'how_are_you', 'joke', 'like', 'name', 'netflix', 'quick_run', 'real_bot', 'sky_net_no', 'sky_net_yes', 'thanks', 'your_thoughts']
113 unique lemmatized words ['a', 'about', 'address', 'age', 'alive', 'am', 'any', 'anyone', 'are', 'awesome', 'awful', 'bad', 'badly', 'best', 'better', 'bye', 'call', 'can', 'chill', 'conscious', 'cool', 'cya', 'day', 'do', 'doe', 'doing', 'dumb', 'favorite', 'film', 'friend', 'funny', 'going', 'good', 'goodbye', 'got', 'great', 'haha', 'happy', 'hate', 'have', 'hello', 'hey', 'hi', 'horrible', 'how', 'human', 'i', 'im', 'indeed', 'is', 'it', 'joke', 'just', 'later', 'leaving', 'like', 'love', 'me', 'mean', 'movie', 'my', 'nah', 'name', 'netflix', 'no', 'not', 'old', 'on', 'one', 'owe', 'real', 'really', 'robot', 'run', 'sad', 'scary', 'see', 'sentient', 'should', 'show', 'singularity', 'stupid', 'sup', 'tell', 'terminator', 'terrible', 'thank', 'thanks', 'thankyou', 'that', 'thats', 'the', 'there', 'think', 'thought', 'trash', 'tv', 'ty', 'up', 'very', 'wa', 'wait', 'watch', 'well', 'what', 'whats', 'why', 'yeah', 'yep', 'yes', 'you', 'your', 'yup']
Training data created
Epoch 1/172
/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  "The `lr` argument is deprecated, use `learning_rate` instead.")
20/20 [==============================] - 0s 2ms/step - loss: 3.0846 - accuracy: 0.0000e+00
Epoch 2/172
20/20 [==============================] - 0s 1ms/step - loss: 3.0208 - accuracy: 0.0909
Epoch 3/172
20/20 [==============================] - 0s 1ms/step - loss: 2.9439 - accuracy: 0.1212
Epoch 4/172
20/20 [==============================] - 0s 1ms/step - loss: 2.9144 - accuracy: 0.1818
Epoch 5/172
20/20 [==============================] - 0s 1ms/step - loss: 2.8364 - accuracy: 0.1818
Epoch 6/172
20/20 [==============================] - 0s 2ms/step - loss: 2.7810 - accuracy: 0.2626
Epoch 7/172
20/20 [==============================] - 0s 2ms/step - loss: 2.6855 - accuracy: 0.2424
Epoch 8/172
20/20 [==============================] - 0s 1ms/step - loss: 2.6125 - accuracy: 0.2626
Epoch 9/172
20/20 [==============================] - 0s 1ms/step - loss: 2.5020 - accuracy: 0.3333
Epoch 10/172
20/20 [==============================] - 0s 1ms/step - loss: 2.5003 - accuracy: 0.3131
Epoch 11/172
20/20 [==============================] - 0s 1ms/step - loss: 2.3364 - accuracy: 0.2828
Epoch 12/172
20/20 [==============================] - 0s 1ms/step - loss: 2.1534 - accuracy: 0.3737
Epoch 13/172
20/20 [==============================] - 0s 1ms/step - loss: 2.1262 - accuracy: 0.3737
Epoch 14/172
20/20 [==============================] - 0s 2ms/step - loss: 1.9807 - accuracy: 0.4242
Epoch 15/172
20/20 [==============================] - 0s 1ms/step - loss: 1.8969 - accuracy: 0.3939
Epoch 16/172
20/20 [==============================] - 0s 2ms/step - loss: 1.9010 - accuracy: 0.4444
Epoch 17/172
20/20 [==============================] - 0s 1ms/step - loss: 1.5404 - accuracy: 0.5354
Epoch 18/172
20/20 [==============================] - 0s 1ms/step - loss: 1.7040 - accuracy: 0.5152
Epoch 19/172
20/20 [==============================] - 0s 1ms/step - loss: 1.4928 - accuracy: 0.5859
Epoch 20/172
20/20 [==============================] - 0s 1ms/step - loss: 1.3550 - accuracy: 0.6162
Epoch 21/172
20/20 [==============================] - 0s 1ms/step - loss: 1.4144 - accuracy: 0.5556
Epoch 22/172
20/20 [==============================] - 0s 1ms/step - loss: 1.2069 - accuracy: 0.6061
Epoch 23/172
20/20 [==============================] - 0s 2ms/step - loss: 1.2246 - accuracy: 0.6667
Epoch 24/172
20/20 [==============================] - 0s 2ms/step - loss: 1.1219 - accuracy: 0.6970
Epoch 25/172
20/20 [==============================] - 0s 1ms/step - loss: 0.9910 - accuracy: 0.6970
Epoch 26/172
20/20 [==============================] - 0s 1ms/step - loss: 1.0872 - accuracy: 0.6566
Epoch 27/172
20/20 [==============================] - 0s 1ms/step - loss: 0.8809 - accuracy: 0.7778
Epoch 28/172
20/20 [==============================] - 0s 1ms/step - loss: 0.9908 - accuracy: 0.7071
Epoch 29/172
20/20 [==============================] - 0s 2ms/step - loss: 1.0446 - accuracy: 0.6162
Epoch 30/172
20/20 [==============================] - 0s 1ms/step - loss: 0.7827 - accuracy: 0.8081
Epoch 31/172
20/20 [==============================] - 0s 1ms/step - loss: 0.8053 - accuracy: 0.7475
Epoch 32/172
20/20 [==============================] - 0s 1ms/step - loss: 0.8125 - accuracy: 0.7273
Epoch 33/172
20/20 [==============================] - 0s 1ms/step - loss: 0.7438 - accuracy: 0.7879
Epoch 34/172
20/20 [==============================] - 0s 1ms/step - loss: 0.5887 - accuracy: 0.8788
Epoch 35/172
20/20 [==============================] - 0s 2ms/step - loss: 0.6117 - accuracy: 0.8485
Epoch 36/172
20/20 [==============================] - 0s 2ms/step - loss: 0.6368 - accuracy: 0.7677
Epoch 37/172
20/20 [==============================] - 0s 1ms/step - loss: 0.5981 - accuracy: 0.8182
Epoch 38/172
20/20 [==============================] - 0s 2ms/step - loss: 0.7278 - accuracy: 0.7576
Epoch 39/172
20/20 [==============================] - 0s 2ms/step - loss: 0.6306 - accuracy: 0.8081
Epoch 40/172
20/20 [==============================] - 0s 2ms/step - loss: 0.4401 - accuracy: 0.8990
Epoch 41/172
20/20 [==============================] - 0s 2ms/step - loss: 0.5734 - accuracy: 0.8687
Epoch 42/172
20/20 [==============================] - 0s 1ms/step - loss: 0.6175 - accuracy: 0.7879
Epoch 43/172
20/20 [==============================] - 0s 1ms/step - loss: 0.5522 - accuracy: 0.8485
Epoch 44/172
20/20 [==============================] - 0s 2ms/step - loss: 0.4856 - accuracy: 0.8889
Epoch 45/172
20/20 [==============================] - 0s 2ms/step - loss: 0.3404 - accuracy: 0.9192
Epoch 46/172
20/20 [==============================] - 0s 2ms/step - loss: 0.4894 - accuracy: 0.8687
Epoch 47/172
20/20 [==============================] - 0s 1ms/step - loss: 0.4380 - accuracy: 0.8788
Epoch 48/172
20/20 [==============================] - 0s 2ms/step - loss: 0.4437 - accuracy: 0.8889
Epoch 49/172
20/20 [==============================] - 0s 1ms/step - loss: 0.3588 - accuracy: 0.9192
Epoch 50/172
20/20 [==============================] - 0s 2ms/step - loss: 0.3107 - accuracy: 0.9192
Epoch 51/172
20/20 [==============================] - 0s 1ms/step - loss: 0.3092 - accuracy: 0.9091
Epoch 52/172
20/20 [==============================] - 0s 2ms/step - loss: 0.3445 - accuracy: 0.8889
Epoch 53/172
20/20 [==============================] - 0s 2ms/step - loss: 0.3238 - accuracy: 0.8889
Epoch 54/172
20/20 [==============================] - 0s 1ms/step - loss: 0.4659 - accuracy: 0.8687
Epoch 55/172
20/20 [==============================] - 0s 1ms/step - loss: 0.2351 - accuracy: 0.9394
Epoch 56/172
20/20 [==============================] - 0s 2ms/step - loss: 0.2952 - accuracy: 0.9293
Epoch 57/172
20/20 [==============================] - 0s 2ms/step - loss: 0.2841 - accuracy: 0.8990
Epoch 58/172
20/20 [==============================] - 0s 2ms/step - loss: 0.3151 - accuracy: 0.9091
Epoch 59/172
20/20 [==============================] - 0s 1ms/step - loss: 0.4168 - accuracy: 0.8384
Epoch 60/172
20/20 [==============================] - 0s 1ms/step - loss: 0.3237 - accuracy: 0.8889
Epoch 61/172
20/20 [==============================] - 0s 1ms/step - loss: 0.3876 - accuracy: 0.8687
Epoch 62/172
20/20 [==============================] - 0s 1ms/step - loss: 0.2289 - accuracy: 0.9596
Epoch 63/172
20/20 [==============================] - 0s 2ms/step - loss: 0.2969 - accuracy: 0.9192
Epoch 64/172
20/20 [==============================] - 0s 2ms/step - loss: 0.2627 - accuracy: 0.9495
Epoch 65/172
20/20 [==============================] - 0s 2ms/step - loss: 0.2946 - accuracy: 0.9091
Epoch 66/172
20/20 [==============================] - 0s 2ms/step - loss: 0.2911 - accuracy: 0.9192
Epoch 67/172
20/20 [==============================] - 0s 2ms/step - loss: 0.2718 - accuracy: 0.9394
Epoch 68/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1761 - accuracy: 0.9495
Epoch 69/172
20/20 [==============================] - 0s 1ms/step - loss: 0.2285 - accuracy: 0.9394
Epoch 70/172
20/20 [==============================] - 0s 1ms/step - loss: 0.3404 - accuracy: 0.8687
Epoch 71/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1757 - accuracy: 0.9596
Epoch 72/172
20/20 [==============================] - 0s 1ms/step - loss: 0.2294 - accuracy: 0.9293
Epoch 73/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1831 - accuracy: 0.9293
Epoch 74/172
20/20 [==============================] - 0s 2ms/step - loss: 0.2823 - accuracy: 0.9394
Epoch 75/172
20/20 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.9192
Epoch 76/172
20/20 [==============================] - 0s 1ms/step - loss: 0.2604 - accuracy: 0.9394
Epoch 77/172
20/20 [==============================] - 0s 1ms/step - loss: 0.2315 - accuracy: 0.9293
Epoch 78/172
20/20 [==============================] - 0s 2ms/step - loss: 0.3404 - accuracy: 0.8990
Epoch 79/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1912 - accuracy: 0.9697
Epoch 80/172
20/20 [==============================] - 0s 1ms/step - loss: 0.2440 - accuracy: 0.9192
Epoch 81/172
20/20 [==============================] - 0s 2ms/step - loss: 0.2304 - accuracy: 0.9293
Epoch 82/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.9394
Epoch 83/172
20/20 [==============================] - 0s 2ms/step - loss: 0.2257 - accuracy: 0.8990
Epoch 84/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1868 - accuracy: 0.9495
Epoch 85/172
20/20 [==============================] - 0s 1ms/step - loss: 0.2380 - accuracy: 0.9293
Epoch 86/172
20/20 [==============================] - 0s 1ms/step - loss: 0.2426 - accuracy: 0.9293
Epoch 87/172
20/20 [==============================] - 0s 1ms/step - loss: 0.2341 - accuracy: 0.9192
Epoch 88/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1967 - accuracy: 0.9596
Epoch 89/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1949 - accuracy: 0.9596
Epoch 90/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1544 - accuracy: 0.9697
Epoch 91/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1767 - accuracy: 0.9293
Epoch 92/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1151 - accuracy: 0.9798
Epoch 93/172
20/20 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9798
Epoch 94/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9798
Epoch 95/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1176 - accuracy: 0.9798
Epoch 96/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1708 - accuracy: 0.9596
Epoch 97/172
20/20 [==============================] - 0s 1ms/step - loss: 0.2153 - accuracy: 0.9495
Epoch 98/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1340 - accuracy: 0.9596
Epoch 99/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9798
Epoch 100/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1202 - accuracy: 0.9394
Epoch 101/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1497 - accuracy: 0.9596
Epoch 102/172
20/20 [==============================] - 0s 2ms/step - loss: 0.2682 - accuracy: 0.8889
Epoch 103/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1664 - accuracy: 0.9596
Epoch 104/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.9495
Epoch 105/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.9596
Epoch 106/172
20/20 [==============================] - 0s 1ms/step - loss: 0.2079 - accuracy: 0.9495
Epoch 107/172
20/20 [==============================] - 0s 1ms/step - loss: 0.2101 - accuracy: 0.9293
Epoch 108/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1751 - accuracy: 0.9091
Epoch 109/172
20/20 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.9899
Epoch 110/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1281 - accuracy: 0.9596
Epoch 111/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1623 - accuracy: 0.9394
Epoch 112/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.9596
Epoch 113/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1476 - accuracy: 0.9394
Epoch 114/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1838 - accuracy: 0.9394
Epoch 115/172
20/20 [==============================] - 0s 1ms/step - loss: 0.2293 - accuracy: 0.9394
Epoch 116/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1327 - accuracy: 0.9596
Epoch 117/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9596
Epoch 118/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1014 - accuracy: 0.9798
Epoch 119/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1203 - accuracy: 0.9394
Epoch 120/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.9798
Epoch 121/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.9495
Epoch 122/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1260 - accuracy: 0.9596
Epoch 123/172
20/20 [==============================] - 0s 2ms/step - loss: 0.0871 - accuracy: 0.9798
Epoch 124/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1040 - accuracy: 0.9697
Epoch 125/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1101 - accuracy: 0.9596
Epoch 126/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1354 - accuracy: 0.9495
Epoch 127/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1579 - accuracy: 0.9596
Epoch 128/172
20/20 [==============================] - 0s 2ms/step - loss: 0.0868 - accuracy: 0.9596
Epoch 129/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1734 - accuracy: 0.9394
Epoch 130/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1288 - accuracy: 0.9697
Epoch 131/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1419 - accuracy: 0.9697
Epoch 132/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1275 - accuracy: 0.9495
Epoch 133/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1422 - accuracy: 0.9596
Epoch 134/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1581 - accuracy: 0.9495
Epoch 135/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1939 - accuracy: 0.9495
Epoch 136/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.9596
Epoch 137/172
20/20 [==============================] - 0s 2ms/step - loss: 0.0724 - accuracy: 0.9899
Epoch 138/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1303 - accuracy: 0.9596
Epoch 139/172
20/20 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9798
Epoch 140/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1055 - accuracy: 0.9596
Epoch 141/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9495
Epoch 142/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.9495
Epoch 143/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1291 - accuracy: 0.9596
Epoch 144/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1541 - accuracy: 0.9596
Epoch 145/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1712 - accuracy: 0.9495
Epoch 146/172
20/20 [==============================] - 0s 2ms/step - loss: 0.0786 - accuracy: 0.9798
Epoch 147/172
20/20 [==============================] - 0s 2ms/step - loss: 0.0937 - accuracy: 0.9596
Epoch 148/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1050 - accuracy: 0.9697
Epoch 149/172
20/20 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.9798
Epoch 150/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1121 - accuracy: 0.9596
Epoch 151/172
20/20 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9697
Epoch 152/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1673 - accuracy: 0.9495
Epoch 153/172
20/20 [==============================] - 0s 2ms/step - loss: 0.0658 - accuracy: 0.9798
Epoch 154/172
20/20 [==============================] - 0s 1ms/step - loss: 0.2128 - accuracy: 0.9293
Epoch 155/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1209 - accuracy: 0.9798
Epoch 156/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1150 - accuracy: 0.9495
Epoch 157/172
20/20 [==============================] - 0s 2ms/step - loss: 0.0647 - accuracy: 1.0000
Epoch 158/172
20/20 [==============================] - 0s 2ms/step - loss: 0.0594 - accuracy: 0.9798
Epoch 159/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1118 - accuracy: 0.9596
Epoch 160/172
20/20 [==============================] - 0s 1ms/step - loss: 0.0641 - accuracy: 0.9798
Epoch 161/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1033 - accuracy: 0.9495
Epoch 162/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1287 - accuracy: 0.9394
Epoch 163/172
20/20 [==============================] - 0s 2ms/step - loss: 0.0995 - accuracy: 0.9697
Epoch 164/172
20/20 [==============================] - 0s 2ms/step - loss: 0.0590 - accuracy: 0.9798
Epoch 165/172
20/20 [==============================] - 0s 2ms/step - loss: 0.0915 - accuracy: 0.9697
Epoch 166/172
20/20 [==============================] - 0s 1ms/step - loss: 0.1580 - accuracy: 0.9495
Epoch 167/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1350 - accuracy: 0.9596
Epoch 168/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1525 - accuracy: 0.9293
Epoch 169/172
20/20 [==============================] - 0s 2ms/step - loss: 0.0706 - accuracy: 0.9798
Epoch 170/172
20/20 [==============================] - 0s 2ms/step - loss: 0.0539 - accuracy: 1.0000
Epoch 171/172
20/20 [==============================] - 0s 2ms/step - loss: 0.1624 - accuracy: 0.9192
Epoch 172/172
20/20 [==============================] - 0s 2ms/step - loss: 0.0582 - accuracy: 0.9798
model created
!pip install ibm_watson
Collecting ibm_watson
  Downloading ibm-watson-5.2.3.tar.gz (406 kB)
     |████████████████████████████████| 406 kB 5.4 MB/s 
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
    Preparing wheel metadata ... done
Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from ibm_watson) (2.8.2)
Collecting ibm-cloud-sdk-core==3.*,>=3.3.6
  Downloading ibm-cloud-sdk-core-3.11.3.tar.gz (45 kB)
     |████████████████████████████████| 45 kB 3.1 MB/s 
Requirement already satisfied: requests<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from ibm_watson) (2.23.0)
Collecting websocket-client==1.1.0
  Downloading websocket_client-1.1.0-py2.py3-none-any.whl (68 kB)
     |████████████████████████████████| 68 kB 7.0 MB/s 
Collecting PyJWT<3.0.0,>=2.0.1
  Downloading PyJWT-2.1.0-py3-none-any.whl (16 kB)
Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.5.3->ibm_watson) (1.15.0)
Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (3.0.4)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (2.10)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (2021.5.30)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (1.24.3)
Building wheels for collected packages: ibm-watson, ibm-cloud-sdk-core
  Building wheel for ibm-watson (PEP 517) ... done
  Created wheel for ibm-watson: filename=ibm_watson-5.2.3-py3-none-any.whl size=403336 sha256=57b1024f909b4d609bb72679b9e97ef07b88e56a0891372df8d8f3ea3c643d41
  Stored in directory: /root/.cache/pip/wheels/93/bc/4e/50facb1865141c8ee96092137b8e2f8495f04c1555f5ae5b36
  Building wheel for ibm-cloud-sdk-core (setup.py) ... done
  Created wheel for ibm-cloud-sdk-core: filename=ibm_cloud_sdk_core-3.11.3-py3-none-any.whl size=74751 sha256=998d7af65a5a7527918daf76e8363a5cb40d1d169cb83c3b6ac02d0c19b98464
  Stored in directory: /root/.cache/pip/wheels/93/85/b2/7c2428617824a175293f7957d4a20b4975eb0ddbef52800006
Successfully built ibm-watson ibm-cloud-sdk-core
Installing collected packages: PyJWT, websocket-client, ibm-cloud-sdk-core, ibm-watson
Successfully installed PyJWT-2.1.0 ibm-cloud-sdk-core-3.11.3 ibm-watson-5.2.3 websocket-client-1.1.0
import anvil.server
anvil.server.connect("NC6JCRR6RSHGSOARZDFGN2Y4-KX6WX2KBRC7MDYSC")
import json
from ibm_watson import ToneAnalyzerV3
from ibm_cloud_sdk_core.authenticators import IAMAuthenticator
msg = list()
text = str()
@anvil.server.callable
def responsed(msg1):
    msg.append(msg1)
    ints = predict_class(msg1)
    res = getResponse(ints, intents)
    return res
# # IBM Tone analyzer
# import json
# from ibm_watson import ToneAnalyzerV3
# from ibm_cloud_sdk_core.authenticators import IAMAuthenticator

# authenticator = IAMAuthenticator("acgtQdBTLh1ul15KYi-6bfbP5EGtSC1OVrM6wgLooiLp")
# tone_analyzer = ToneAnalyzerV3(
#     version='2017-09-21',
#     authenticator=authenticator
# )

# tone_analyzer.set_service_url( "https://api.eu-gb.tone-analyzer.watson.cloud.ibm.com/instances/77cd4a81-e420-4212-85bc-3f4c4ae6dd4a")
# text = " "
# for i in msg:
#     text = text+i
# tone_analysis = tone_analyzer.tone(
#     {'text': text},
#     content_type='application/json'
# ).get_result()
# print(json.dumps(tone_analysis, indent=2))
@anvil.server.callable
def song_emotion():
    authenticator = IAMAuthenticator("acgtQdBTLh1ul15KYi-6bfbP5EGtSC1OVrM6wgLooiLp")
    tone_analyzer = ToneAnalyzerV3(
        version='2017-09-21',
        authenticator=authenticator
    
    )

    tone_analyzer.set_service_url( "https://api.eu-gb.tone-analyzer.watson.cloud.ibm.com/instances/77cd4a81-e420-4212-85bc-3f4c4ae6dd4a")
    # text = ""
    # for i in msg:
    #     text = text+i
    len1 = len(msg)
    tone_analysis = tone_analyzer.tone(
        {'text': msg[len1-1]+" "+msg[len1-2]+" "+msg[len1-3]+" "+msg[len1-4]+" "+msg[len1-5]},
        content_type='application/json'
    ).get_result()
    dic1 = dict()
    emotion=tone_analysis["document_tone"]["tones"][0]["tone_name"]
    dic1['emotion'] = emotion
    import requests

    url=f"http://ws.audioscrobbler.com/2.0/?method=tag.gettoptracks&tag={emotion}&api_key=dc767f78f7b91f7b55d01c13fb684e31&format=json&limit=10"
    response = requests.get(url)
    payload = response.json()
    for i in range(10):
        r=payload['tracks']['track'][i]
        dic1[r['name']] = r['url']
    return dic1
# print(msg)
# # print(text)
# print(len(msg))
# SONG RECOMMENDATION
import requests

url=f"http://ws.audioscrobbler.com/2.0/?method=tag.gettoptracks&tag=happy&api_key=dc767f78f7b91f7b55d01c13fb684e31&format=json&limit=5"
response = requests.get(url)
payload = response.json()
# for i in range(4):
r=payload['tracks']['track'][0]
# print(r['url'])
print(payload)
{'tracks': {'track': [{'name': 'Pumped Up Kicks', 'duration': '236', 'mbid': '816b3284-5f24-4f3a-9554-750e0bf5d060', 'url': 'https://www.last.fm/music/Foster+the+People/_/Pumped+Up+Kicks', 'streamable': {'#text': '0', 'fulltrack': '0'}, 'artist': {'name': 'Foster the People', 'mbid': 'e0e1a584-dd0a-4bd1-88d1-c4c62895039d', 'url': 'https://www.last.fm/music/Foster+the+People'}, 'image': [{'#text': 'https://lastfm.freetls.fastly.net/i/u/34s/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'small'}, {'#text': 'https://lastfm.freetls.fastly.net/i/u/64s/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'medium'}, {'#text': 'https://lastfm.freetls.fastly.net/i/u/174s/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'large'}, {'#text': 'https://lastfm.freetls.fastly.net/i/u/300x300/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'extralarge'}], '@attr': {'rank': '1'}}, {'name': "I'm Yours", 'duration': '242', 'mbid': 'a5a2330e-2fff-4601-a715-6e68a8e98fbf', 'url': 'https://www.last.fm/music/Jason+Mraz/_/I%27m+Yours', 'streamable': {'#text': '0', 'fulltrack': '0'}, 'artist': {'name': 'Jason Mraz', 'mbid': '82eb8936-7bf6-4577-8320-a2639465206d', 'url': 'https://www.last.fm/music/Jason+Mraz'}, 'image': [{'#text': 'https://lastfm.freetls.fastly.net/i/u/34s/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'small'}, {'#text': 'https://lastfm.freetls.fastly.net/i/u/64s/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'medium'}, {'#text': 'https://lastfm.freetls.fastly.net/i/u/174s/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'large'}, {'#text': 'https://lastfm.freetls.fastly.net/i/u/300x300/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'extralarge'}], '@attr': {'rank': '2'}}, {'name': 'Dog Days Are Over', 'duration': '270', 'mbid': '772ea437-45b3-4868-b3c0-b80526ef0fa3', 'url': 'https://www.last.fm/music/Florence+%252B+the+Machine/_/Dog+Days+Are+Over', 'streamable': {'#text': '0', 'fulltrack': '0'}, 'artist': {'name': 'Florence + the Machine', 'mbid': '5fee3020-513b-48c2-b1f7-4681b01db0c6', 'url': 'https://www.last.fm/music/Florence+%252B+the+Machine'}, 'image': [{'#text': 'https://lastfm.freetls.fastly.net/i/u/34s/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'small'}, {'#text': 'https://lastfm.freetls.fastly.net/i/u/64s/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'medium'}, {'#text': 'https://lastfm.freetls.fastly.net/i/u/174s/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'large'}, {'#text': 'https://lastfm.freetls.fastly.net/i/u/300x300/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'extralarge'}], '@attr': {'rank': '3'}}, {'name': 'A-Punk', 'duration': '136', 'mbid': '047db898-cedf-48a0-afc2-5b686c9b8840', 'url': 'https://www.last.fm/music/Vampire+Weekend/_/A-Punk', 'streamable': {'#text': '0', 'fulltrack': '0'}, 'artist': {'name': 'Vampire Weekend', 'mbid': 'af37c51c-0790-4a29-b995-456f98a6b8c9', 'url': 'https://www.last.fm/music/Vampire+Weekend'}, 'image': [{'#text': 'https://lastfm.freetls.fastly.net/i/u/34s/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'small'}, {'#text': 'https://lastfm.freetls.fastly.net/i/u/64s/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'medium'}, {'#text': 'https://lastfm.freetls.fastly.net/i/u/174s/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'large'}, {'#text': 'https://lastfm.freetls.fastly.net/i/u/300x300/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'extralarge'}], '@attr': {'rank': '4'}}, {'name': "Friday I'm in Love", 'duration': '214', 'mbid': '8d9104c6-7d8e-460b-8c3d-2d797f79953d', 'url': 'https://www.last.fm/music/The+Cure/_/Friday+I%27m+in+Love', 'streamable': {'#text': '0', 'fulltrack': '0'}, 'artist': {'name': 'The Cure', 'mbid': '69ee3720-a7cb-4402-b48d-a02c366f2bcf', 'url': 'https://www.last.fm/music/The+Cure'}, 'image': [{'#text': 'https://lastfm.freetls.fastly.net/i/u/34s/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'small'}, {'#text': 'https://lastfm.freetls.fastly.net/i/u/64s/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'medium'}, {'#text': 'https://lastfm.freetls.fastly.net/i/u/174s/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'large'}, {'#text': 'https://lastfm.freetls.fastly.net/i/u/300x300/2a96cbd8b46e442fc41c2b86b821562f.png', 'size': 'extralarge'}], '@attr': {'rank': '5'}}], '@attr': {'tag': 'happy', 'page': '1', 'perPage': '5', 'totalPages': '4502', 'total': '22510'}}}
# print(dic1)
print("Chatbot : Hey there, Wassup ?")
# responded function takes text of user and returns chatbot output
for i in range(5):
    m = input("User : ")
    res = responsed(m)
    print("Chatbot : "+res)
ans = song_emotion()
print("Emotion : "+ans['emotion'])
Chatbot : Hey there, Wassup ?
User : Today is my birthday, what is your age?
Chatbot : I'm a robot I dont have an age...
User : oh! I am in a good mood today, how are you
Chatbot : I've never been better, how are you?
User : I am good, you are nice
Chatbot : Youre pretty cool yourself!
User : Thank you, what is your name
Chatbot : I dont have a name yet but I was thinking maybe Alex. That has a nice ring to it dont you think?
User : yes
Chatbot : Yep, I like how it sounds. I got it from the Terminator.
Emotion : Joy
# song_emotion function would return dictionary consisting of emotion and recommended songs
ans = song_emotion()
print("Emotion : "+ans['emotion'])
ans.pop('emotion')
lst = list(ans.keys())
print("Song Recommendations : ")
for i in range(10):
    print("Song_name : "+lst[i])
    print("Song_URL : "+ans[lst[i]])
Emotion : Joy
Song Recommendations : 
Song_name : Butter
Song_URL : https://www.last.fm/music/BTS/_/Butter
Song_name : You And Your Heart
Song_URL : https://www.last.fm/music/Jack+Johnson/_/You+And+Your+Heart
Song_name : Yoshimi Battles The Pink Robots Part 1
Song_URL : https://www.last.fm/music/The+Flaming+Lips/_/Yoshimi+Battles+The+Pink+Robots+Part+1
Song_name : Come on! Feel the Illinoise!
Song_URL : https://www.last.fm/music/Sufjan+Stevens/_/Come+on%21+Feel+the+Illinoise%21
Song_name : praise you - radio edit
Song_URL : https://www.last.fm/music/Fatboy+Slim/_/praise+you+-+radio+edit
Song_name : Let's Go
Song_URL : https://www.last.fm/music/Matt+&+Kim/_/Let%27s+Go
Song_name : Old Old Fashioned
Song_URL : https://www.last.fm/music/Frightened+Rabbit/_/Old+Old+Fashioned
Song_name : Lake Shore Drive
Song_URL : https://www.last.fm/music/Aliotta+Haynes+Jeremiah/_/Lake+Shore+Drive
Song_name : My Little Town
Song_URL : https://www.last.fm/music/Simon+&+Garfunkel/_/My+Little+Town
Song_name : Esa Noche
Song_URL : https://www.last.fm/music/Caf%C3%A9+Tacvba/_/Esa+Noche
